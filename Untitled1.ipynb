{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOqQOHyH4TRkaOE5z85zskC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lumos129/Lumos129/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQJtOe0k70lb",
        "outputId": "88c614d9-2aff-4ca1-851d-225e52b07891"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import jieba.analyse\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "class HW1(object):\n",
        "    def __init__(self):\n",
        "        #dataset_txt = pd.read_csv('hw1-dataset.txt',\n",
        "        dataset_txt = pd.read_csv('https://raw.githubusercontent.com/cjwu/cjwu.github.io/master/courses/nlp/hw1-dataset.txt',\n",
        "                                  encoding='utf-8',\n",
        "                                  header=None, sep='\\r\\n',\n",
        "                                  engine='python',\n",
        "                                  error_bad_lines=False)\n",
        "        print(dataset_txt)\n",
        "        self.dataset_values = dataset_txt.values.tolist()\n",
        "        #print(self.dataset_values[0:2])\n",
        "\n",
        "        #self.dataset_values = self.dataset_values[0:20]\n",
        "\n",
        "        self.stop_word = [' ', ',', '.', '?', '\\t', '..'] + [str(d) for d in range(10)]\n",
        "\n",
        "    def parse(self):\n",
        "        for idx in range(len(self.dataset_values)):\n",
        "            string_line = self.dataset_values[idx][0]\n",
        "            #print(string_line)\n",
        "            #item_list = jieba.analyse.extract_tags(string_line)\n",
        "            item_list = jieba.lcut(string_line, cut_all=False, HMM=True)\n",
        "            #print(item_list)\n",
        "\n",
        "            self.dataset_values[idx] = ' '.join(item_list)\n",
        "            #print(self.dataset_values[idx])\n",
        "\n",
        "        print(self.dataset_values[-10:])\n",
        "\n",
        "    def cut(self):\n",
        "        #jieba.set_dictionary(dict.txt)\n",
        "        for idx in range(len(self.dataset_values)):\n",
        "            string_line = self.dataset_values[idx][0]\n",
        "            items_list = jieba.lcut(string_line, cut_all=False, HMM=True)\n",
        "            #print(items_list)\n",
        "\n",
        "            #self.dataset_values[idx] = [item for item in items_list if item not in self.stop_word]\n",
        "            self.dataset_values[idx] = [item for item in items_list if len(item) > 1]\n",
        "            #print(self.dataset_values[idx])\n",
        "\n",
        "        print(self.dataset_values[0:10])\n",
        "\n",
        "    def cut2(self):\n",
        "        self.dataset_cut = []\n",
        "        for line in self.dataset_values[0:10]:\n",
        "            string_line = line[0]\n",
        "            items_list = jieba.lcut(string_line, cut_all=False, HMM=True)\n",
        "\n",
        "            self.dataset_cut.append([item for item in items_list if item not in self.stop_word])\n",
        "\n",
        "        print(self.dataset_cut[0:10])\n",
        "\n",
        "    def tf(self):\n",
        "        # 构建词汇表\n",
        "        self.vocabulary = {}\n",
        "        for paper in self.dataset_values:\n",
        "            for term in paper:\n",
        "                if term in self.vocabulary:\n",
        "                    self.vocabulary[term] += 1\n",
        "                else:\n",
        "                    self.vocabulary[term] = 1\n",
        "\n",
        "        print(self.vocabulary.items())\n",
        "\n",
        "        # 排序\n",
        "        self.top100 = sorted(self.vocabulary.items(), key=lambda k: (k[1]), reverse=True)[0:100]\n",
        "        print(self.top100)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    hw = HW1()\n",
        "    hw.cut()\n",
        "    hw.tf()\n",
        "\n",
        "\n",
        "    exit(0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#mydata_txt = pd.read_csv('https://raw.githubusercontent.com/cjwu/cjwu.github.io/master/courses/nlp/hw1-dataset.txt',\n",
        "                         #encoding='utf-8',\n",
        "                         #error_bad_lines=False)\n",
        "\n",
        "#mydata_txt = pd.read_csv('hw1-dataset.txt', encoding='utf-8', header=None)\n",
        "#print(mydata_txt)\n",
        "\n",
        "#with open('https://raw.githubusercontent.com/cjwu/cjwu.github.io/master/courses/nlp/hw1-dataset.txt', 'rt', ) as f:\n",
        "#    print(f.readlines())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                                                        0\n",
            "0       為什麼 聖結石 會被酸而 這群人 不會？\\t質感 劇本 成員 都差很多好嗎 不要拿腎結石來污...\n",
            "1                    為什麼慶祝228會被罵可是慶端午不會？\\t因為屈原不是台灣人，是楚國人。\n",
            "2                           有沒有戰神阿瑞斯的八卦?\\t爵士就是阿瑞斯 男主角最後死了\n",
            "3                     理論與實務最脫節的系\\t哪個系不脫節...你問最不脫節的簡單多了...\n",
            "4                            為什麼PTT這麼多人看棒球\\t肥宅才看棒球　系壘一堆胖子\n",
            "...                                                   ...\n",
            "418197                              我試過完美放棄　的確很踏實\\t只能縮我酥惹\n",
            "418198            司法官 準備口試是不是表示錄取了？\\t我們是作法官二十年養好人脈去作律師，悲哀\n",
            "418199           有沒有柯名陽和劉明彰的風向的八卦？\\t超哥退出新竹也還好吧, 去台中接老劉的班阿\n",
            "418200                            在youtube上直播音樂?\\t昨天半夜問過了\n",
            "418201                       有人也在等steam冬季特賣嗎？\\t好遊戲一堆PS4獨佔\n",
            "\n",
            "[418202 rows x 1 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 0.816 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[['什麼', '聖結石', '這群', '不會', '質感', '劇本', '成員', '很多', '不要', '結石來', '污辱', '這群'], ['什麼', '慶祝', '228', '可是', '端午', '不會', '因為', '屈原', '不是', '台灣', '楚國人'], ['戰神', '阿瑞斯', '八卦', '爵士', '就是', '阿瑞斯', '男主角', '最後死'], ['理論', '實務', '脫節', '哪個', '脫節', '...', '你問', '脫節', '簡單', '...'], ['什麼', 'PTT', '這麼', '棒球', '肥宅', '棒球', '系壘', '一堆', '胖子'], ['什麼', '達摩祖', '師傳', '那麼', '好看', '達摩', '從頭', '被動', '別人問', '問題'], ['3D', '小畫家', '有人', '會畫', '3D', '小當家', '有人', '會畫'], ['天龍', '人來', '宜蘭', '南部', '東部', '他國', '事務', '..'], ['機車', '推出', 'uber', '計程', '機車會', '怎樣', '載到', '肥宅會', '痛苦'], ['台中', '龍邦', '世貿', '有人', '跳樓', '曾經', '當過', '全台', '第一', '高樓', '可惜', '不到', '一年']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}